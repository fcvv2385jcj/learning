## 												softmax分类问题

---

### 数据预处理

#### 缺失值问题

原因：数据包缺少部分数据

处理方法：直接丢失可能会造成重要信息的损失。可以采用平均值，中值总数等填充

参考：[(32条消息) 缺失值的处理方法_zrjdds的博客-CSDN博客_缺失值](https://blog.csdn.net/zrjdds/article/details/50223091?ops_request_misc=&request_id=&biz_id=102&utm_term=缺失值&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-50223091.142^v2^es_vector_control_group,143^v4^control&spm=1018.2226.3001.4187)

####  独热编码

为什么用独热编码：cnn网络输出层通常为softmax，而softmax输出是一个概率分布，因此为了达到学习的目的，输入的标签也需要是一个特征分布。而one-hot编码恰好就反映了输入标签的概率分布。

参考：[(35条消息) 数据预处理之独热编码（One-Hot）：为什么要使用one-hot编码？_weixin_30377461的博客-CSDN博客](https://blog.csdn.net/weixin_30377461/article/details/97668210?ops_request_misc=%7B%22request%5Fid%22%3A%22164890436016780271513017%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164890436016780271513017&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-1-97668210.142^v5^pc_search_insert_es_download,157^v4^control&utm_term=+为什么要独热编码？&spm=1018.2226.3001.4187)

---

#### 处理异常值

方法：箱型图分析，3δ原则。用3δ原则判断概率极小的值，并删除异常数值。

参考：[(33条消息) 数据清洗- Pandas 清洗“脏”数据（一）_Zhang,Xuewen的博客-CSDN博客_数据清洗](https://blog.csdn.net/weixin_35702861/article/details/83094537?ops_request_misc={"request_id"%3A"164756647516782089314987"%2C"scm"%3A"20140713.130102334.."}&request_id=164756647516782089314987&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-83094537.142^v2^es_vector_control_group,143^v4^control&utm_term=数据清洗&spm=1018.2226.3001.4187)

---

### 优化器

#### 		momentum:

作用：主要思想是累积之前的梯度，加快学习过程。引入变量v累积之前的梯度，若当前梯度方向与累积梯度一致，则当前梯度加强，下降幅度更大。反之若当前梯度与累积梯度方向不一致，则削弱当前梯度下载幅度。

#### 		RMSprop:

作用：引入s累积梯度动量，消除摆动幅度大的方向，通过之前累积的梯度，削弱或增强当前的梯度变化。

![20170923134334368.png](https://s2.loli.net/2022/03/23/St9vhcLV5TPbmUq.png)

### 正则化:

作用：减低模型复杂度，用于防止过拟合。引入对损失函数的惩罚项，相当于对损失函数的参数增加了一个约束条件，使其不能太过于接近谷底达到过拟合状态。

<img src="https://2022sterben.oss-cn-beijing.aliyuncs.com/img/Dq2.png" alt="正则项的边缘直观表示" style="zoom: 33%;" />



---

### **分类问题评估模型**

​		TP：正样本分类正确

​		TF：负样本分类正确

​		FN：正样本分类为负样本

​		FP：负样本分类为正样本

准确率(*Accuracy*)：

​			测试集中分类正确样本与总样本比值。

​			![](https://img-blog.csdn.net/20180709094035173?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTMwNjMwOTk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

精确率(*Precision*)：

​			预测中真正为正的占预测为正的比值

​			![](https://img-blog.csdn.net/20180709094203518?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTMwNjMwOTk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

召回率(*Recall*)：

​			预测中真正为正的占实际为正的比值

​			![](https://img-blog.csdn.net/2018070909424549?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTMwNjMwOTk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

F1值(*H-mean*)：

​			用于综合考量

​			$F1 = 2Recall*Precision / (Recall + Precision)$

---

## 	**小总结**

​		主要还是解决问题的能力有待提升。考核期间很多问题都是要靠自己解决，那我这周处理一些问题的时候，总是很容易转牛角尖。比如代码有问题，下面明明给了warming，但是我就不看，或者只是匆匆瞄看一眼不去思考一下有可能是什么原因，就马上复制粘贴去查，查到方法了也不想一下，这个方法是做了什么才解决的问题，只是照着方法做就是了。最后1,2个小时过去问题还没解决。有些问题这样解决了之后，回过头来一想，warming上面不就描述的清清楚楚嘛。所以说看到问题还是要想想问什么会出现这样的问题。

​		另外就是要多请教一下别人。计算机会有很多特殊的词语，不知道的就是不知道，如果不知道关键词也很难查。这周在做softmax分类的时候，需要用到“独热编码”，一开始我不知道这个词，查了半天，最后偶然看见的才知道这种处理方法叫这个。然后过了几天，看见有人在群里问这个，然后师兄就回答到了这个词，问答不过1分钟。所以还是要多请教一下，时间宝贵啊。

---

## 爬虫

---

### 基本流程

- 准备工作

  通过浏览器查看分析目标网页。借助开发者工具找到需要爬取的数据的位置。

- 获取数据

  通过HTTP库向目标站点发送请求。需要伪装身份，获取目标站点的源代码。

- 解析内容

  得到的内容可能为HTML，json格式，用页面解析库、正则表达式解析。

- 保存数据

  将提取的信息保存为特定格式。

#### 正则表达式

设置一组符号作为规定，用于判断输入类型是否符合要求。比如判断邮箱地址，身份证，密码账号。在爬取网页中，需要从网页中找到需要爬取的数据的格式，用正则表达式表示。

---

## 线性回归问题

### 回归模型的评估

SSE(和方差，误差平方和) ：The sum of squares due to error

MSE(均方差，方差)：Mean squared error

RMSE(均方根，标准差)：Root mean squared error

R-square(确定系数)：Coefficient of determination（主要用R方来做评估）

---

## 特征工程：

从原始数据中获得更多信息，获得更好的训练数据，使机器学习算法达到最佳性能。

### 	**特征选择**

​	从特征集合中挑选一组最具统计意义的特征集合，达到降维效果。

​	常用方法：

​		filter（刷选器）：计算时间上更高效，但不考虑特征间的相关性。

​		wrapper（封装器）：考虑到特征间的相关性，但在特征数量少时容易过拟合，特征数量多时计算时间	   		更长。

​		Embeded（集成方法）：让学习器自主选择学习特征。

​		特征选择过程包括产生过程，评价函数，停止准则，验证过程。

<img src="https://2022sterben.oss-cn-beijing.aliyuncs.com/img/image-20220329150643558.png" alt="image-20220329150643558" style="zoom: 80%;" />

### 	 **特征提取**

​	将原始数据重新组合特征得到新的特征，达到降维效果。

​	方法：主成分分析（PCA），独立成分分析（ICA），线性判别分析（LDA）可用属于无监督模型实现。

### 	特征构建

​	从原始数据中人工的构建新的特征，将原始数据分出多个特征。

参考：

[(34条消息) 机器学习之特征工程_拾毅者的博客-CSDN博客_特征工程](https://blog.csdn.net/Dream_angel_Z/article/details/49388733?ops_request_misc=%7B%22request%5Fid%22%3A%22164853665416780264091059%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=164853665416780264091059&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-5-49388733.142^v5^pc_search_insert_es_download,143^v6^register&utm_term=特征工程&spm=1018.2226.3001.4187)

---

### K-means聚类算法

​		用于数据的处理，属于无监督学习。将数据集样本划分为若干个不相交的子集，每个子集对应某些特征。K代表取k个样本作为簇中心。means代表取每个聚类中数据的均值作为该簇的中心或称质心，用该质心对该簇进行描述。

实现算法主要四点：1 簇个数的选择   2 计算各样本到簇中心的距离  3 更新簇中心  4 重复2,3直到簇中心不移动

---

### 决策树

​	用于分类问题。对数据的某一特征进行测试，根据检测结果分配到子节点，不断进行递归检测，直到叶节点，最后归类到叶节点的类中。基本思想以信息熵为度量构造一颗熵值下降最快的树，达到最小化损失函数的目的。

​	主要步骤：特征选择、决策树生成、决策树修剪。

---

#### 概率论基础知识

##### 信息熵

​	通过事件所有发生概率计算事件所含信息量的期望。

​	<img src="C:\Users\33903\AppData\Roaming\Typora\typora-user-images\image-20220329201153466.png" alt="image-20220329201153466" style="zoom:80%;" />

三个性质：

单调性，发生概率越高，携带的信息量越小。

非负性。

累加性，多个随机事件同时发生的不确定性量度可以用各事件的不确定性量度的和表示。

##### 条件熵

​	y在x的条件下的信息熵。对于y消除了对x的不确定性。

​	![image-20220329202441507](https://2022sterben.oss-cn-beijing.aliyuncs.com/img/image-20220329202441507.png)

##### 互信息

​	表示两个随机事件的相关性

​	![image-20220329203147221](https://2022sterben.oss-cn-beijing.aliyuncs.com/img/image-20220329203147221.png)

​	表示事件X在减去知道Y事件后仍然不知道的事。

##### 相对熵

​	衡量两个随机变量的差距。

<img src="https://2022sterben.oss-cn-beijing.aliyuncs.com/img/image-20220329210111246.png" alt="image-20220329210111246" style="zoom:80%;" />

