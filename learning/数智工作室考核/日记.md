## 												softmax分类问题

---

## 3/16

#### 缺失值问题:

​		查看每列缺失数，发现如果删除缺失值的那一整行，最终数据只剩下678行，相比原本799行损失了超过10%，最终是采用np中的填充平均值。

#### 参考：[(32条消息) 缺失值的处理方法_zrjdds的博客-CSDN博客_缺失值](https://blog.csdn.net/zrjdds/article/details/50223091?ops_request_misc=&request_id=&biz_id=102&utm_term=缺失值&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-50223091.142^v2^es_vector_control_group,143^v4^control&spm=1018.2226.3001.4187)

---

####  独热编码

​		Sotfmax最终输出的结果y_hat是一个5 * m （**5对应0-4共5种结果的概率，m是样本数）**的矩阵，每个值的范围在[0,1]，但数据集中的y是一个m*1的矩阵。因此需要做一个转化。一开始用的是一个for循环后来才知道这叫独热编码(one hot)，可以用numpy实现。

#### 参考：[(32条消息) python之独热编码的实现_大彤小忆的博客-CSDN博客_python 独热编码的实现](https://blog.csdn.net/HUAI_BI_TONG/article/details/108179403?ops_request_misc={"request_id"%3A"164751174216780264055885"%2C"scm"%3A"20140713.130102334.."}&request_id=164751174216780264055885&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-3-108179403.142^v2^es_vector_control_group,143^v4^control&utm_term=python实现独热编码&spm=1018.2226.3001.4187)

[吴恩达《深度学习》L2W3作业 - Heywhale.com](https://www.heywhale.com/mw/project/5dde48f6ca27f8002c4a8396)

---

## 3/17

​		模型虽然构建出来，但w和b基本不更新，cost下降缓慢，最终测试集预测结果全为同一种分类，准确率3%，要不就是梯度消失输出NaN，可能是数据集没处理好，或者模型代码写错。

---

## 3/18

​		解决了数据异常的问题，处理异常值方法：箱型图分析，3δ原则。用3δ原则并删除异常数值。

​		但是还是同样的情况，学习率稍微加大，就会出现nan，cost下降也很慢。后来调试才知道，用的是relu函数，而数据中负数占了很大比例，因此这些数据没有被激活，也就导致了最后预测得到全是相同的结果，因为这些特征根本没被学习到。出现nan的原因是初始化参数时权重w太小，并且用了relu，多次循环后梯度消失。

#### 参考：

[(33条消息) 数据清洗- Pandas 清洗“脏”数据（一）_Zhang,Xuewen的博客-CSDN博客_数据清洗](https://blog.csdn.net/weixin_35702861/article/details/83094537?ops_request_misc={"request_id"%3A"164756647516782089314987"%2C"scm"%3A"20140713.130102334.."}&request_id=164756647516782089314987&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-83094537.142^v2^es_vector_control_group,143^v4^control&utm_term=数据清洗&spm=1018.2226.3001.4187)

[(33条消息) 常用激活函数（Sigmiod、Tanh、Softmax、ReLU、elu、LReLU、Softplus）函数表达式、特点、图像绘制（代码）---已解决_喵喵love的博客-CSDN博客_softmax激活函数图像](https://blog.csdn.net/qq_41603193/article/details/112120889)

---

## 3/19

​		新数据暂时没什么可以处理的，就是数据量比较大，看着像是28*28的图片。之前一直用的4层节点，参考了一些资料后，决定从最简单的2层开始构建softmax模型，用的还是relu，因为这次数据都是大于0。

​		尝试了不同数量的神经元，最后用的m,125,10层，验证集精确到97%，测试集92%，偏差偏高，下一步准备再加一些优化方法，例如adam。

​		使用adam后验证集达到99.9%，测试集提升到96.3%。下一步先搞清楚adam原理，之后再了解特征工程，k-means，决策树，看看还有哪些可以用的，还有模型的评估。

参考：[[双语字幕\]吴恩达深度学习deeplearning.ai_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1FT4y1E74V?p=78)

[吴恩达《深度学习》L2W2作业 - Heywhale.com](https://www.heywhale.com/mw/project/5dd79f80f41512002ceb3ca7)

---

## 3/20

### 		momentum:

​			主要思想是累积之前的梯度，加快学习过程。引入变量v累积之前的梯度，若当前梯度方向与累积梯度一致，则当前梯度加强，下降幅度更大。反之若当前梯度与累积梯度方向不一致，则削弱当前梯度下载幅度。

### 		RMSprop:

​			引入s累积梯度动量，消除摆动幅度大的方向，通过之前累积的梯度，削弱或增强当前的梯度变化。

![20170923134334368.png](https://s2.loli.net/2022/03/23/St9vhcLV5TPbmUq.png)

#### **正则化:**

​			减低模型复杂度，用于防止过拟合。引入对损失函数的惩罚项，用于调节模型参数w,减低模型复杂度。

---

##  3/21

#### **面向对象**：

​	封装：隐藏实现的细节，只留下一个借口给用户使用。

​	继承：在一个类的基础上建立新的类。

​	多态：父类经过子类重写，会出现多个版本。

**类(class)：**将数据和对数据的操作封装。

​	抽象类：不能够创建对象的类，包含抽象方法。

​	init:类的初始化，在创建类的实例时调用

​	私有属性：表示无法从外部访问。

​	私有方法：表示无法从外部访问，只能从内部调用。

---

## 3/23

​		学习git操作，以及用picgo图床通过阿里云上传图片，并且关联typora。

---

## 3/22

#### 	**评估模型**

​		TP：正样本分类正确

​		TF：负样本分类正确

​		FN：正样本分类为负样本

​		FP：负样本分类为正样本

​		准确率(*Accuracy*)：测试集中分类正确样本与总样本比值。

​			![](https://img-blog.csdn.net/20180709094035173?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTMwNjMwOTk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

​		精确率(*Precision*)：预测中真正为正的占预测为正的比值

​			![](https://img-blog.csdn.net/20180709094203518?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTMwNjMwOTk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

​		召回率(Recall)：预测中真正为正的占实际为正的比值

​			![](https://img-blog.csdn.net/2018070909424549?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTMwNjMwOTk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

​		F1值(H-mean)：用于综合考量

​			$F1 = 2Recall*Precision / (Recall + Precision)$

---

## 	3/25

​		总结一下这几周的问题。

​		主要还是解决问题的能力有待提升。考核期间很多问题都是要靠自己解决，那我这周处理一些问题的时候，总是很容易转牛角尖。比如代码有问题，下面明明给了warming，但是我就不看，或者只是匆匆瞄看一眼不去思考一下有可能是什么原因，就马上复制粘贴去查，查到方法了也不想一下，这个方法是做了什么才解决的问题，只是照着方法做就是了。最后1,2个小时过去问题还没解决。有些问题这样解决了之后，回过头来一想，warming上面不就描述的清清楚楚嘛。所以说看到问题还是要想想问什么会出现这样的问题。

​		另外就是要多请教一下别人。计算机会有很多特殊的词语，不知道的就是不知道，如果不知道关键词也很难查。这周在做softmax分类的时候，需要用到“独热编码”，一开始我不知道这个词，查了半天，最后偶然看见的才知道这种处理方法叫这个。然后过了几天，看见有人在群里问这个，然后师兄就回答到了这个词，问答不过1分钟。所以还是要多请教一下，时间宝贵啊。

